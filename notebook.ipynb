{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "fqe4ksp37xh4a6x6b5ii",
   "authorId": "5810802233931",
   "authorName": "AFERO",
   "authorEmail": "allie.fero@snowflake.com",
   "sessionId": "bee8238f-363e-4f7a-9d0f-3422be1a20c4",
   "lastEditTime": 1749253301730
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e3fdc8-8480-41c1-8d8a-6f7eb7101ad7",
   "metadata": {
    "name": "Setup",
    "collapsed": false
   },
   "source": "# Initialize Environment for Distributed Processing"
  },
  {
   "cell_type": "code",
   "id": "c0c0dd10-192c-4f79-940c-ca675028b32c",
   "metadata": {
    "language": "python",
    "name": "cell19",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Some of the features in this HOL are in preview. Pin the version of snowflake-ml-python for reproducibility. \n# NOTE - you do not need to restart the kernel since we're running this before importing snowflake-ml-python.\n!pip install snowflake-ml-python==1.8.3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3300af7-7d13-43e6-b05b-0b20d3ffa5ce",
   "metadata": {
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": "!pip freeze | grep snowflake",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91117e3b-e018-45ba-a547-0001558b0c5a",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "import ray\nimport logging\nlogging.getLogger().setLevel(logging.WARNING)\n\n\ncontext = ray.data.DataContext.get_current()\ncontext.execution_options.verbose_progress = False\ncontext.enable_operator_progress_bars = False\ncontext.enable_progress_bars = False",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "601cf1ec-6b27-4604-9f81-119455b1d4d5",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "from snowflake.ml.runtime_cluster import scale_cluster\n\n# Scale out the notebook to have multiple nodes available for execution\nSCALE_FACTOR = 2\nscale_cluster(SCALE_FACTOR)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9a0ea63-361a-49b8-a001-f70ecfa800cd",
   "metadata": {
    "name": "Process_Reviews",
    "collapsed": false
   },
   "source": "# Process Review Text Data\n- Load reviews with `SFStageTextDataSource`\n- Parse review text with Ray data"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": true,
    "codeCollapsed": false
   },
   "source": "# use ray data to process sentiment \nfrom snowflake.ml.ray.datasource import SFStageTextDataSource\n\nfile_name = \"*.txt\"\nstage_name = \"REVIEWS\"\n\ntext_source = SFStageTextDataSource(\n    stage_location=stage_name,\n    file_pattern=file_name\n)\n\ntext_dataset = ray.data.read_datasource(text_source)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bdfae7c2-bc34-4a8d-a86f-e4a145e647e6",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "def parse_reviews(batch):\n    \"\"\"\n    Parse reviews to extract UUID and review text from the input string.\n    \n    Args:\n        batch: Dictionary containing 'text' and 'file_name' keys\n        \n    Returns:\n        Dictionary with parsed UUID and review text\n    \"\"\"\n    # Initialize empty dictionary for results\n    parsed_data = {}\n    \n    value = batch[\"text\"]\n    # Split on the first occurrence of comma\n    parts = value.split('\",\"', 1)\n    \n    # Clean up the UUID (remove leading/trailing quotes)\n    uuid = parts[0].strip('\"')\n    \n    # Clean up the review text (remove trailing quote)\n    review_text = parts[1].rstrip('\"')\n    \n    # Store parsed values\n    parsed_data['UUID'] = uuid\n    parsed_data['REVIEW_TEXT'] = review_text\n        \n    return parsed_data\n\n# Apply the parsing function to the dataset\nparsed_dataset = text_dataset.map(parse_reviews)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41fae9e1-749d-4675-bb64-2ead05c798ab",
   "metadata": {
    "name": "Review_Quality",
    "collapsed": false
   },
   "source": "# Predict Review Quality\n- Predict the quality with one-shot classification via HF pipeline"
  },
  {
   "cell_type": "code",
   "id": "4a9fdb90-1bae-4216-b910-b413f78e1b09",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "from transformers import pipeline\nimport numpy as np\n\n\nclass ModelPredictor:\n    def __init__(self):\n        # Load model\n        self.classifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\n\n    # define your batch operations    \n    def __call__(self, batch):\n        candidate_labels = ['detailed with specific information and experience', 'basic accurate information', 'generic brief with no details']\n        resp = self.classifier(batch[\"REVIEW_TEXT\"].tolist(), candidate_labels)\n\n        # Handle both resp and batch results\n        if isinstance(resp, dict):\n            raise ValueError(f\"Expected batch response, got {resp} for batch {batch['REVIEW_TEXT']}\")\n            \n        # Add results to batch\n        batch[\"REVIEW_QUALITY\"] = np.array([result[\"labels\"][np.argmax(result[\"scores\"])] for result in resp])\n        \n\n        return batch\n\n# Apply batch operations to your dataset. HF Pipeline is itself a batch operation, so we use Ray data just to scale across nodes, setting concurrency to number of nodes we have started.\ndataset = parsed_dataset.map_batches(ModelPredictor, concurrency=SCALE_FACTOR, batch_size=10, num_cpus=25)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa4b3b9c-3430-45db-92dc-bb1b29eae790",
   "metadata": {
    "name": "Store_Reviews",
    "collapsed": false
   },
   "source": "# Store Processed Data in Snowflake"
  },
  {
   "cell_type": "code",
   "id": "2424713b-83ff-43b9-9f62-d3ba3ce82bdf",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "from snowflake.ml.ray.datasink.table_data_sink import SnowflakeTableDatasink\n\ndatasink = SnowflakeTableDatasink(\n    table_name=\"REVIEWS\",\n    auto_create_table=True,\n    override=False,\n    )\ndataset.write_datasink(datasink)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6557fb5c-2bf2-4795-8d9c-c01bd7e09012",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "show tables;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "147ea8ea-1ee3-48f2-8a39-55beedc463ee",
   "metadata": {
    "name": "Sentiment_Analysis",
    "collapsed": false
   },
   "source": "# Execute Sentiment Analysis"
  },
  {
   "cell_type": "code",
   "id": "10972b8f-f098-40fe-b498-94f6f8233bf0",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "ALTER TABLE\n  REVIEWS\nADD\n  COLUMN if not exists REVIEW_SENTIMENT FLOAT\n  /* Add the REVIEW_SENTIMENT column */;\n  /* Update the table with sentiment analysis */\nUPDATE\n  REVIEWS\nSET\n  REVIEW_SENTIMENT = SNOWFLAKE.CORTEX.SENTIMENT (REVIEW_TEXT);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0411bbba-4bc9-4d3d-a869-5833ed3e23f8",
   "metadata": {
    "language": "sql",
    "name": "cell25"
   },
   "outputs": [],
   "source": "select * from reviews limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3732afef-283a-4fdc-a8fa-90842c17194c",
   "metadata": {
    "name": "Feature_Engineering",
    "collapsed": false
   },
   "source": "# Prepare Data for Model Training"
  },
  {
   "cell_type": "code",
   "id": "52b76e74-8991-4458-b5c9-9a6fec345c80",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "tabular_data = session.table(\"TABULAR_DATA\")\nreview_data = session.table(\"REVIEWS\")\n\ntrain_dataframe = tabular_data.join(\n    review_data,\n    review_data['UUID'] == tabular_data['UUID'],\n    'inner'\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "026e49b9-f72f-4741-9785-5929bf139d76",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": "train_dataframe.count()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98d96873-358c-4b71-8a36-c7386f72b27f",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "train_dataframe.columns",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7c8e4a8-60d0-43bd-bc5c-45b1218e5dee",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "# Encode review sentiment and review quality\nfrom snowflake.ml.modeling.preprocessing import LabelEncoder\n\n# Select the columns to encode\ncolumns_to_encode = [\"REVIEW_QUALITY\", \"PRODUCT_LAYOUT\"]\n\n# Initialize LabelEncoder for each column\nencoders = [LabelEncoder(input_cols=[col], output_cols=[f\"{col}_OUT\"]) for col in columns_to_encode]\nfor encoder in encoders:\n    train_dataframe = encoder.fit(train_dataframe).transform(train_dataframe)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8276baac-1886-4b14-a43d-15c039c49173",
   "metadata": {
    "name": "Distributed_Training",
    "collapsed": false
   },
   "source": "# Train an XGBoost Model\n- Trains an XGBoost model over two nodes using Snowflake distributed `XGBEstimator`"
  },
  {
   "cell_type": "code",
   "id": "b40f4747-43c7-4601-ba2a-55353b7743b8",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\nfrom snowflake.ml.data.data_connector import DataConnector\n\nINPUT_COLS = [\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\nLABEL_COL = 'PURCHASE_DECISION'\n\nparams = {\n    \"eta\": 0.1,\n    \"max_depth\": 8,\n    \"min_child_weight\": 100,\n    \"tree_method\": \"hist\",\n}\n\nscaling_config = XGBScalingConfig(\n    use_gpu=False\n)\n\nestimator = XGBEstimator(\n    n_estimators=50,\n    objective=\"reg:squarederror\",\n    params=params,\n    scaling_config=scaling_config,\n)\n\n\ndc = DataConnector.from_dataframe(train_dataframe)\nxgb_model = estimator.fit(\n    dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "487da425-c275-4067-ba7d-ac358a4edb4a",
   "metadata": {
    "name": "Register_And_Deploy",
    "collapsed": false
   },
   "source": "# Register and Deploy the Model\n- Register model to Snowflake Model Registry\n- Deploy code outside of notebook using ML Jobs"
  },
  {
   "cell_type": "code",
   "id": "1dc05873-face-4b13-adb5-239fa1d93437",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import registry\nreg = registry.Registry(session=session)\n\n# Log the model in Snowflake Model Registry\nmodel_ref = reg.log_model(\n    model_name=\"deployed_xgb\",\n    model=xgb_model,\n    conda_dependencies=[\"scikit-learn\",\"xgboost\"],\n    sample_input_data=train_dataframe.select(INPUT_COLS),\n    comment=\"XGBoost model for forecasting customer demand\",\n    options= {\"enable_explainability\": True},\n    target_platforms = [\"WAREHOUSE\"]\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "075ba043-4622-43a9-aab6-e59a76adaa86",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": "# Now that we're done processing data, scale back down\nscale_cluster(1, is_async=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a71509e0-021f-49e0-9d9f-02049c37a2eb",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "from snowflake.ml.jobs import remote\n@remote(compute_pool=\"HOL_COMPUTE_POOL_HIGHMEM\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\ndef update_reviews():\n    import ray\n    from snowflake.ml.ray.datasink.table_data_sink import SnowflakeTableDatasink\n    from snowflake.ml.ray.datasource import SFStageTextDataSource\n    \n    file_name = \"*.txt\"\n    stage_name = \"REVIEWS\"\n    \n    text_source = SFStageTextDataSource(\n        stage_location=stage_name,\n        file_pattern=file_name\n    )\n    \n    text_dataset = ray.data.read_datasource(text_source)\n\n    # text_dataset = ray.data.read_datasource(text_source)\n    parsed_dataset = text_dataset.map(parse_reviews)\n    dataset = parsed_dataset.map_batches(ModelPredictor, concurrency=1, batch_size=10, num_cpus=24)\n\n    datasink = SnowflakeTableDatasink(\n        table_name=\"REVIEWS\",\n        auto_create_table=True,\n        override=False,\n        )\n    dataset.write_datasink(datasink)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35d2400a-ae2d-4371-a5d8-f20e1df96729",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "# Create a training job\n@remote(compute_pool=\"HOL_COMPUTE_POOL_HIGHMEM\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\ndef retrain():\n    import datetime\n    from snowflake.snowpark.context import get_active_session\n    from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n    from snowflake.ml.data.data_connector import DataConnector\n\n    session = get_active_session()\n\n    tabular_data = session.table(\"HOL_DB.HOL_SCHEMA.TABULAR_DATA\")\n    review_data = session.table(\"HOL_DB.HOL_SCHEMA.REVIEWS\")\n        \n    INPUT_COLS = [\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\n    LABEL_COL = 'PURCHASE_DECISION'\n    \n    train_dataframe = tabular_data.join(\n        review_data,\n        review_data['UUID'] == tabular_data['UUID'],\n        'inner'\n    )\n\n    # Encode review sentiment and review quality\n    from snowflake.ml.modeling.preprocessing import LabelEncoder\n    \n    # Select the columns to encode\n    columns_to_encode = [\"REVIEW_QUALITY\", \"PRODUCT_LAYOUT\"]\n    \n    # Initialize LabelEncoder for each column\n    encoders = [LabelEncoder(input_cols=[col], output_cols=[f\"{col}_OUT\"]) for col in columns_to_encode]\n    for encoder in encoders:\n        train_dataframe = encoder.fit(train_dataframe).transform(train_dataframe)\n        \n    params = {\n        \"eta\": 0.1,\n        \"max_depth\": 8,\n        \"min_child_weight\": 100,\n        \"tree_method\": \"hist\",\n    }\n    \n    scaling_config = XGBScalingConfig(\n        use_gpu=False\n    )\n    \n    estimator = XGBEstimator(\n        n_estimators=50,\n        objective=\"reg:squarederror\",\n        params=params,\n        scaling_config=scaling_config,\n    )\n    \n    \n    dc = DataConnector.from_dataframe(train_dataframe)\n    xgb_model = estimator.fit(\n        dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n    )\n    \n    dc = DataConnector.from_dataframe(train_dataframe)\n    xgb_model = estimator.fit(\n        dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n    )\n\n    from snowflake.ml.registry import registry\n    reg = registry.Registry(session=session)\n    \n    # Log the model in Snowflake Model Registry\n    _ = reg.log_model(\n        model_name=\"CONVERSTION_CLASSIFIER\",\n        model=xgb_model,\n        version_name=f\"retrain_{datetime.datetime.now().strftime('v%Y%m%d_%H%M%S')}\",\n        conda_dependencies=[\"scikit-learn\",\"xgboost\"],\n        sample_input_data=train_dataframe.select(INPUT_COLS),\n        comment=\"XGBoost model for forecasting customer demand\",\n        options= {\"enable_explainability\": True},\n        target_platforms = [\"WAREHOUSE\"]\n    )\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4effe789-c052-4dd4-b28f-8614915f1bcf",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "# You can run the job manually, and get the status and logs of the job\ntrain_job = retrain()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4748787c-ccdd-47c6-a5eb-5bebe1c5006a",
   "metadata": {
    "language": "python",
    "name": "cell31",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "while train_job.status == \"PENDING\":\n    time.sleep(1)\n\n# Once job starts running, we can view the logs\ntrain_job.get_logs()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb5731e7-6495-4801-bb66-df0f6689ca68",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "# we can also see all the jobs, and manage them with the job manager\nfrom snowflake.ml import jobs\n\nall_jobs = jobs.list_jobs().to_pandas()\n\njob_manager = jobs.manager\n\nmask = all_jobs['status'].str.contains(\"FAILED\")\nfiltered_df = all_jobs[mask]\n\njob_ids = filtered_df[\"id\"]\nfor id in job_ids:\n    job_manager.delete_job(id)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "483b0c5f-e594-4dfc-be95-ebfd2076f71d",
   "metadata": {
    "name": "Automate_Pipeline",
    "collapsed": false
   },
   "source": "# Create Automated ML Pipeline\n- Automate the deployment of the pipeline using Snowflake Tasks\n- After DAG creation, navigate to Monitoring -> Task History to view execution"
  },
  {
   "cell_type": "code",
   "id": "d4a3b669-d13e-41f6-8106-93eda2de7ad1",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "from snowflake.core.task.dagv1 import DAG, DAGTask\nfrom snowflake.core.task.context import TaskContext\nfrom datetime import timedelta\nfrom snowflake.snowpark import Session\nimport snowflake.ml.jobs.manager as manager\nimport datetime\nimport json\n\nWAREHOUSE = session.get_current_warehouse()\n\n\ndef refresh_reviews(session: Session) -> None:\n    job = update_reviews()\n    # Throw error if job fails\n    final_status = job.wait()\n\n    if final_status == \"FAILED\":\n        raise RuntimeError(f\"Job {job} failed with logs \")\n\ndef update_sentiment(session: Session) -> None:\n    sql_text = \"\"\"\n        UPDATE\n          REVIEWS\n        SET\n          REVIEW_SENTIMENT = SNOWFLAKE.CORTEX.SENTIMENT (REVIEW_TEXT);\n    \"\"\"\n    session.sql(sql_text).collect()\n\ndef retrain_model(session: Session) -> None:\n    job = retrain()\n    # Throw error if job fails\n    final_status = job.wait()\n\n    if final_status == \"FAILED\":\n        raise RuntimeError(f\"Job {job} failed with logs \")\n\ndef setup(session: Session) -> str:\n    info = dict(\n        run_id=datetime.datetime.now().strftime(\"v%Y%m%d_%H%M%S\"),\n    )\n    return json.dumps(info)\n\ndef create_dag() -> DAG:\n    with DAG(\n        \"review_model_dag\",\n        warehouse=WAREHOUSE,\n        schedule=timedelta(days=1),\n        stage_location=\"payload_stage\",\n        packages=[\"snowflake-snowpark-python\", \"snowflake-ml-python==1.8.2\", \"transformers\"]\n    ) as dag:\n        # Need to wrap first function in a DAGTask to make >> operator work properly\n        setup_task = DAGTask(\"setup\", definition=setup)\n\n        # Build the DAG\n        setup_task >> refresh_reviews >> update_sentiment >> retrain_model\n\n    return dag\n\nfrom snowflake.core import CreateMode, Root\nfrom snowflake.core.task.dagv1 import DAGOperation\napi_root = Root(session)\n\ndag_op = DAGOperation(\n    schema=api_root.databases[session.get_current_database()].schemas[session.get_current_schema()]\n)\n\ndag = create_dag()\ndag_op.deploy(dag, mode=CreateMode.or_replace)\ndag_op.run(dag)\n\ncurrent_runs = dag_op.get_current_dag_runs(dag)\nfor r in current_runs:\n    print(f\"RunId={r.run_id} State={r.state}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39463ae9-e36a-4593-bc0d-4870a76837b8",
   "metadata": {
    "name": "Feature_Importance",
    "collapsed": false
   },
   "source": "# Assess Feature Importance with Explainability"
  },
  {
   "cell_type": "code",
   "id": "530a9b3c-f4d1-4181-ba03-de8365a3f07e",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": "show models;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc512677-adf6-4705-bbba-7baa273cdf37",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "explanations = model_ref.run(train_dataframe.select(INPUT_COLS), function_name=\"explain\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66b0eb01-c66c-4d42-958b-937b56d3f4c0",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "explanations",
   "execution_count": null
  }
 ]
}