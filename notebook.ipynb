{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91117e3b-e018-45ba-a547-0001558b0c5a",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "context = ray.data.DataContext.get_current()\n",
    "context.execution_options.verbose_progress = False\n",
    "context.enable_operator_progress_bars = False\n",
    "context.enable_progress_bars = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cf1ec-6b27-4604-9f81-119455b1d4d5",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.runtime_cluster import scale_cluster\n",
    "\n",
    "SCALE_FACTOR = 2\n",
    "scale_cluster(SCALE_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": true,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# use ray data to process sentiment \n",
    "from snowflake.ml.ray.datasource import SFStageTextDataSource\n",
    "\n",
    "file_name = \"*.txt\"\n",
    "stage_name = \"REVIEWS\"\n",
    "\n",
    "text_source = SFStageTextDataSource(\n",
    "    stage_location=stage_name,\n",
    "    file_pattern=file_name\n",
    ")\n",
    "\n",
    "text_dataset = ray.data.read_datasource(text_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfae7c2-bc34-4a8d-a86f-e4a145e647e6",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "def parse_reviews(batch):\n",
    "    \"\"\"\n",
    "    Parse reviews to extract UUID and review text from the input string.\n",
    "    \n",
    "    Args:\n",
    "        batch: Dictionary containing 'text' and 'file_name' keys\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with parsed UUID and review text\n",
    "    \"\"\"\n",
    "    # Initialize empty dictionary for results\n",
    "    parsed_data = {}\n",
    "    \n",
    "    value = batch[\"text\"]\n",
    "    # Split on the first occurrence of comma\n",
    "    parts = value.split('\",\"', 1)\n",
    "    \n",
    "    # Clean up the UUID (remove leading/trailing quotes)\n",
    "    uuid = parts[0].strip('\"')\n",
    "    \n",
    "    # Clean up the review text (remove trailing quote)\n",
    "    review_text = parts[1].rstrip('\"')\n",
    "    \n",
    "    # Store parsed values\n",
    "    parsed_data['UUID'] = uuid\n",
    "    parsed_data['REVIEW_TEXT'] = review_text\n",
    "        \n",
    "    return parsed_data\n",
    "\n",
    "# Apply the parsing function to the dataset\n",
    "parsed_dataset = text_dataset.map(parse_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fdb90-1bae-4216-b910-b413f78e1b09",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ModelPredictor:\n",
    "    def __init__(self):\n",
    "        # Load model\n",
    "        self.classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # define your batch operations    \n",
    "    def __call__(self, batch):\n",
    "        candidate_labels = ['detailed with specific information and experience', 'basic accurate information', 'generic brief with no details']\n",
    "        resp = self.classifier(batch[\"REVIEW_TEXT\"].tolist(), candidate_labels)\n",
    "\n",
    "        # Handle both resp and batch results\n",
    "        if isinstance(resp, dict):\n",
    "            raise ValueError(f\"Expected batch response, got {resp} for batch {batch['REVIEW_TEXT']}\")\n",
    "            \n",
    "        # Add results to batch\n",
    "        batch[\"REVIEW_QUALITY\"] = np.array([result[\"labels\"][np.argmax(result[\"scores\"])] for result in resp])\n",
    "        \n",
    "\n",
    "        return batch\n",
    "\n",
    "# Apply batch operations to your dataset. HF Pipeline is itself a batch operation, so we use Ray data just to scale across nodes, setting concurrency to number of nodes we have started.\n",
    "dataset = parsed_dataset.map_batches(ModelPredictor, concurrency=SCALE_FACTOR, batch_size=10, num_cpus=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424713b-83ff-43b9-9f62-d3ba3ce82bdf",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.ray.datasink.table_data_sink import SnowflakeTableDatasink\n",
    "\n",
    "datasink = SnowflakeTableDatasink(\n",
    "    table_name=\"REVIEWS\",\n",
    "    auto_create_table=True,\n",
    "    override=False,\n",
    "    )\n",
    "dataset.write_datasink(datasink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557fb5c-2bf2-4795-8d9c-c01bd7e09012",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "show tables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10972b8f-f098-40fe-b498-94f6f8233bf0",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "ALTER TABLE\n",
    "  REVIEWS\n",
    "ADD\n",
    "  COLUMN if not exists REVIEW_SENTIMENT FLOAT\n",
    "  /* Add the REVIEW_SENTIMENT column */;\n",
    "  /* Update the table with sentiment analysis */\n",
    "UPDATE\n",
    "  REVIEWS\n",
    "SET\n",
    "  REVIEW_SENTIMENT = SNOWFLAKE.CORTEX.SENTIMENT (REVIEW_TEXT);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411bbba-4bc9-4d3d-a869-5833ed3e23f8",
   "metadata": {
    "language": "sql",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "select * from reviews limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b76e74-8991-4458-b5c9-9a6fec345c80",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "tabular_data = session.table(\"TABULAR_DATA\")\n",
    "review_data = session.table(\"REVIEWS\")\n",
    "\n",
    "train_dataframe = tabular_data.join(\n",
    "    review_data,\n",
    "    review_data['UUID'] == tabular_data['UUID'],\n",
    "    'inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e49b9-f72f-4741-9785-5929bf139d76",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
    "train_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d96873-358c-4b71-8a36-c7386f72b27f",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "train_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8e4a8-60d0-43bd-bc5c-45b1218e5dee",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "# Encode review sentiment and review quality\n",
    "from snowflake.ml.modeling.preprocessing import LabelEncoder\n",
    "\n",
    "# Select the columns to encode\n",
    "columns_to_encode = [\"REVIEW_QUALITY\", \"PRODUCT_LAYOUT\"]\n",
    "\n",
    "# Initialize LabelEncoder for each column\n",
    "encoders = [LabelEncoder(input_cols=[col], output_cols=[f\"{col}_OUT\"]) for col in columns_to_encode]\n",
    "for encoder in encoders:\n",
    "    train_dataframe = encoder.fit(train_dataframe).transform(train_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f4747-43c7-4601-ba2a-55353b7743b8",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "\n",
    "INPUT_COLS = [\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\n",
    "LABEL_COL = 'PURCHASE_DECISION'\n",
    "\n",
    "# {'REVIEW_ID', 'REVIEW_SENTIMENT', 'REVIEW_QUALITY', 'PRODUCT_LAYOUT', 'PRODUCT_LAYOUT_OUT', 'PAGE_LOAD_TIME', 'PRODUCT_RATING', 'ID', 'PURCHASE_DECISION', 'REVIEW_TEXT', 'PRODUCT_TYPE', 'UUID'}\n",
    "\n",
    "params = {\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 8,\n",
    "    \"min_child_weight\": 100,\n",
    "    \"tree_method\": \"hist\",\n",
    "}\n",
    "\n",
    "scaling_config = XGBScalingConfig(\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "estimator = XGBEstimator(\n",
    "    n_estimators=50,\n",
    "    objective=\"reg:squarederror\",\n",
    "    params=params,\n",
    "    scaling_config=scaling_config,\n",
    ")\n",
    "\n",
    "\n",
    "dc = DataConnector.from_dataframe(train_dataframe)\n",
    "xgb_model = estimator.fit(\n",
    "    dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc05873-face-4b13-adb5-239fa1d93437",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import registry\n",
    "reg = registry.Registry(session=session)\n",
    "\n",
    "# Log the model in Snowflake Model Registry\n",
    "model_ref = reg.log_model(\n",
    "    model_name=\"deployed_xgb\",\n",
    "    model=xgb_model,\n",
    "    conda_dependencies=[\"scikit-learn\",\"xgboost\"],\n",
    "    sample_input_data=train_dataframe.select(INPUT_COLS),\n",
    "    comment=\"XGBoost model for forecasting customer demand\",\n",
    "    options= {\"enable_explainability\": True},\n",
    "    target_platforms = [\"WAREHOUSE\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ba043-4622-43a9-aab6-e59a76adaa86",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": [
    "# Now that we're done processing data, scale back down\n",
    "scale_cluster(1, is_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71509e0-021f-49e0-9d9f-02049c37a2eb",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.jobs import remote\n",
    "@remote(compute_pool=\"HOL_COMPUTE_POOL_HIGHMEM\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\n",
    "def update_reviews():\n",
    "    import ray\n",
    "    from snowflake.ml.ray.datasink.table_data_sink import SnowflakeTableDatasink\n",
    "    from snowflake.ml.ray.datasource import SFStageTextDataSource\n",
    "    \n",
    "    file_name = \"*.txt\"\n",
    "    stage_name = \"REVIEWS\"\n",
    "    \n",
    "    text_source = SFStageTextDataSource(\n",
    "        stage_location=stage_name,\n",
    "        file_pattern=file_name\n",
    "    )\n",
    "    \n",
    "    text_dataset = ray.data.read_datasource(text_source)\n",
    "\n",
    "    # text_dataset = ray.data.read_datasource(text_source)\n",
    "    parsed_dataset = text_dataset.map(parse_reviews)\n",
    "    dataset = parsed_dataset.map_batches(ModelPredictor, concurrency=1, batch_size=10, num_cpus=24)\n",
    "\n",
    "    datasink = SnowflakeTableDatasink(\n",
    "        table_name=\"REVIEWS\",\n",
    "        auto_create_table=True,\n",
    "        override=False,\n",
    "        )\n",
    "    dataset.write_datasink(datasink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2400a-ae2d-4371-a5d8-f20e1df96729",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "# Create a training job\n",
    "@remote(compute_pool=\"HOL_COMPUTE_POOL_HIGHMEM\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\n",
    "def retrain():\n",
    "    import datetime\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
    "    from snowflake.ml.data.data_connector import DataConnector\n",
    "\n",
    "    session = get_active_session()\n",
    "\n",
    "    tabular_data = session.table(\"HOL_DB.HOL_SCHEMA.TABULAR_DATA\")\n",
    "    review_data = session.table(\"HOL_DB.HOL_SCHEMA.REVIEWS\")\n",
    "        \n",
    "    INPUT_COLS = [\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\n",
    "    LABEL_COL = 'PURCHASE_DECISION'\n",
    "    \n",
    "    train_dataframe = tabular_data.join(\n",
    "        review_data,\n",
    "        review_data['UUID'] == tabular_data['UUID'],\n",
    "        'inner'\n",
    "    )\n",
    "\n",
    "    # Encode review sentiment and review quality\n",
    "    from snowflake.ml.modeling.preprocessing import LabelEncoder\n",
    "    \n",
    "    # Select the columns to encode\n",
    "    columns_to_encode = [\"REVIEW_QUALITY\", \"PRODUCT_LAYOUT\"]\n",
    "    \n",
    "    # Initialize LabelEncoder for each column\n",
    "    encoders = [LabelEncoder(input_cols=[col], output_cols=[f\"{col}_OUT\"]) for col in columns_to_encode]\n",
    "    for encoder in encoders:\n",
    "        train_dataframe = encoder.fit(train_dataframe).transform(train_dataframe)\n",
    "        \n",
    "    params = {\n",
    "        \"eta\": 0.1,\n",
    "        \"max_depth\": 8,\n",
    "        \"min_child_weight\": 100,\n",
    "        \"tree_method\": \"hist\",\n",
    "    }\n",
    "    \n",
    "    scaling_config = XGBScalingConfig(\n",
    "        use_gpu=False\n",
    "    )\n",
    "    \n",
    "    estimator = XGBEstimator(\n",
    "        n_estimators=50,\n",
    "        objective=\"reg:squarederror\",\n",
    "        params=params,\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    dc = DataConnector.from_dataframe(train_dataframe)\n",
    "    xgb_model = estimator.fit(\n",
    "        dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
    "    )\n",
    "    \n",
    "    dc = DataConnector.from_dataframe(train_dataframe)\n",
    "    xgb_model = estimator.fit(\n",
    "        dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
    "    )\n",
    "\n",
    "    from snowflake.ml.registry import registry\n",
    "    reg = registry.Registry(session=session)\n",
    "    \n",
    "    # Log the model in Snowflake Model Registry\n",
    "    _ = reg.log_model(\n",
    "        model_name=\"CONVERSTION_CLASSIFIER\",\n",
    "        model=xgb_model,\n",
    "        version_name=f\"retrain_{datetime.datetime.now().strftime('v%Y%m%d_%H%M%S')}\",\n",
    "        conda_dependencies=[\"scikit-learn\",\"xgboost\"],\n",
    "        sample_input_data=train_dataframe.select(INPUT_COLS),\n",
    "        comment=\"XGBoost model for forecasting customer demand\",\n",
    "        options= {\"enable_explainability\": True},\n",
    "        target_platforms = [\"WAREHOUSE\"]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4effe789-c052-4dd4-b28f-8614915f1bcf",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# You can run the job manually, and get the status and logs of the job\n",
    "train_job = retrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748787c-ccdd-47c6-a5eb-5bebe1c5006a",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "while train_job.status == \"PENDING\":\n",
    "    time.sleep(1)\n",
    "\n",
    "# Once job starts running, we can view the logs\n",
    "train_job.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5731e7-6495-4801-bb66-df0f6689ca68",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# we can also see all the jobs, and manage them with the job manager\n",
    "from snowflake.ml import jobs\n",
    "\n",
    "all_jobs = jobs.list_jobs().to_pandas()\n",
    "\n",
    "job_manager = jobs.manager\n",
    "\n",
    "mask = all_jobs['status'].str.contains(\"FAILED\")\n",
    "filtered_df = all_jobs[mask]\n",
    "\n",
    "job_ids = filtered_df[\"id\"]\n",
    "for id in job_ids:\n",
    "    job_manager.delete_job(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3b669-d13e-41f6-8106-93eda2de7ad1",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "from snowflake.core.task.dagv1 import DAG, DAGTask\n",
    "from snowflake.core.task.context import TaskContext\n",
    "from datetime import timedelta\n",
    "from snowflake.snowpark import Session\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "WAREHOUSE = session.get_current_warehouse()\n",
    "\n",
    "# Warning: This is temporary behavior that will be changed in the future when we have first class support for image pinning. \n",
    "def _update_image_tag():\n",
    "    import snowflake.ml.jobs as jobs\n",
    "    import importlib\n",
    "\n",
    "    jobs._utils.constants.DEFAULT_IMAGE_TAG = '1.2.3'\n",
    "    importlib.reload(jobs)\n",
    "    \n",
    "    print(jobs._utils.constants.DEFAULT_IMAGE_TAG)\n",
    "\n",
    "\n",
    "def refresh_reviews(session: Session) -> None:\n",
    "    _update_image_tag()\n",
    "    job = update_reviews()\n",
    "    # Throw error if job fails\n",
    "    final_status = job.wait()\n",
    "\n",
    "    if final_status == \"FAILED\":\n",
    "        raise RuntimeError(f\"Job {job} failed with logs \")\n",
    "\n",
    "def update_sentiment(session: Session) -> None:\n",
    "    sql_text = \"\"\"\n",
    "        UPDATE\n",
    "          REVIEWS\n",
    "        SET\n",
    "          REVIEW_SENTIMENT = SNOWFLAKE.CORTEX.SENTIMENT (REVIEW_TEXT);\n",
    "    \"\"\"\n",
    "    session.sql(sql_text).collect()\n",
    "\n",
    "def retrain_model(session: Session) -> None:\n",
    "    _update_image_tag()\n",
    "    job = retrain()\n",
    "    # Throw error if job fails\n",
    "    final_status = job.wait()\n",
    "\n",
    "    if final_status == \"FAILED\":\n",
    "        raise RuntimeError(f\"Job {job} failed with logs \")\n",
    "\n",
    "def setup(session: Session) -> str:\n",
    "    info = dict(\n",
    "        run_id=datetime.datetime.now().strftime(\"v%Y%m%d_%H%M%S\"),\n",
    "    )\n",
    "    return json.dumps(info)\n",
    "\n",
    "def create_dag() -> DAG:\n",
    "    with DAG(\n",
    "        \"review_model_dag\",\n",
    "        warehouse=WAREHOUSE,\n",
    "        schedule=timedelta(days=1),\n",
    "        stage_location=\"payload_stage\",\n",
    "        packages=[\"snowflake-snowpark-python\", \"snowflake-ml-python\", \"transformers\"]\n",
    "    ) as dag:\n",
    "        # Need to wrap first function in a DAGTask to make >> operator work properly\n",
    "        setup_task = DAGTask(\"setup\", definition=setup)\n",
    "\n",
    "        # Build the DAG\n",
    "        setup_task >> refresh_reviews >> update_sentiment >> retrain_model\n",
    "\n",
    "    return dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efad23-49db-4ef3-9661-28ef4cbc6445",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "from snowflake.core import CreateMode, Root\n",
    "from snowflake.core.task.dagv1 import DAGOperation\n",
    "api_root = Root(session)\n",
    "\n",
    "dag_op = DAGOperation(\n",
    "    schema=api_root.databases[session.get_current_database()].schemas[session.get_current_schema()]\n",
    ")\n",
    "\n",
    "dag = create_dag()\n",
    "dag_op.deploy(dag, mode=CreateMode.or_replace)\n",
    "dag_op.run(dag)\n",
    "\n",
    "current_runs = dag_op.get_current_dag_runs(dag)\n",
    "for r in current_runs:\n",
    "    print(f\"RunId={r.run_id} State={r.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66976adf-5780-48cc-a298-89b1c4779449",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "current_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a9b3c-f4d1-4181-ba03-de8365a3f07e",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "show models;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc512677-adf6-4705-bbba-7baa273cdf37",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "explanations = model_ref.run(train_dataframe.select(INPUT_COLS), function_name=\"explain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0eb01-c66c-4d42-958b-937b56d3f4c0",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "explanations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "allie.fero@snowflake.com",
   "authorId": "3666053751229",
   "authorName": "ALLIEFERO",
   "lastEditTime": 1747431907384,
   "notebookId": "v3f5tvszzenjpge26mmc",
   "sessionId": "566f9656-3575-4f9e-9c26-e832a2eabb51"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
